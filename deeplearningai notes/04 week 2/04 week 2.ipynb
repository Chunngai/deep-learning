{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>**# 2.3 Residual Networks (ResNets)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very very deep nn are difficult to train because of **vanishing** and **exploding** gradients types of problems.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a^{[l]}$ -> Linear -> ReLU -> $a^{[l + 1]}$ -> Linear -> ReLU -> $a^{[l + 2]}$  \n",
    "  \n",
    "$z^{[l + 1]} = W^{[l + 1]} a^{[l]} + b^{[l + 1]}$  \n",
    "$a^{[l + 1]} = g(z^{[l + 1]})$  # g() is ReLU here  \n",
    "  \n",
    "$z^{[l + 2]} = W^{[l + 2]} a^{[l + 1]} + b^{[l + 2]}$  \n",
    "$a^{[l + 2]} = g(z^{[l + 2]})$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a residual net, we're going to make a change to the steps above. We take $a^{[l]}$ and just fast forward it, copy it much further into the neural network before applying to nonlinearity, the (second here) ReLU nonlinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a^{[l]}$ <font color=red>-></font> Linear -> ReLU -> $a^{[l + 1]}$ -> Linear <font color=red>-></font> ReLU -> $a^{[l + 2]}$    \n",
    "(from the red \"->\" to the red \"->\")\n",
    " \n",
    "  \n",
    "$z^{[l + 1]} = W^{[l + 1]} a^{[l]} + b^{[l + 1]}$  \n",
    "$a^{[l + 1]} = g(z^{[l + 1]})$  # g() is ReLU here  \n",
    "  \n",
    "$z^{[l + 2]} = W^{[l + 2]} a^{[l + 1]} + b^{[l + 2]}$  \n",
    "<font color=blue>$a^{[l + 2]} = g(z^{[l + 2]} + a^{[l]})$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a^{[l]}$ here makes a residual block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using residual blocks allows you to train much deeper neural networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why Resnets?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Std optimization algorithm to train a plain nn (without all the extra residual), as the number of layers increases, the training error will tend to decrease after a while, but then trend back up. In reality, having a plain nn that's very deep means that your optimization algorithm just has much harder time training, the training error gets worse if you pick a nn that's too deep. With Resnets even as the number of layers gets deeper, you can have a performance of the training error kind of keep on going down. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>**# 2.3 Why Resnets work**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the last video we know as the network gets deeper, it can hurt your ability to train the network to do well on the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a nn like this:  \n",
    "  \n",
    "X -> [Big NN] -> $a^{[l]}$  \n",
    "  \n",
    "And add two layers:  \n",
    "X -> [Big NN] -> $a^{[l]}$ <font color=red>-></font> Layer1 -> <font color=red>Layer2</font> -> $a^{[l+2]}$  \n",
    "  \n",
    "And use the ReLU func."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a^{[l + 2]} = g(z^{[l + 2]} + a^{[l]}) = g(W^{[l + 2]} a^{[l + 1]} + b^{[l + 2]} + a^{[l]})$  \n",
    "  \n",
    "If L2 regularization is used, if $W^{[l + 2]}$ = 0, and suppose b = 0,  \n",
    "$a^{[l + 2]} = g(a^{[l]}) = a^{[l]}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this shows is that the identity function is easy for residual block to learn. What that means is that adding these two layers in neural network, it does not really hurt your nn ability to do as well as the simpler nn without the two extra layers, coz it's quite easy for it to learn the identity func. It just copies $a^{[l]}$ to $a^{[l + 2]}$, despite the addition of these two layers. If the layers actually learn something useful, maybe they can do even better than learning the identity func. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main reason the residual network works is that it's so easy for these extra layers to learn the identity func that you're kind of guaranteed that it does not hurt performance and lot of time maybe even helps performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more detial. Assume that $z^{[l + 2]}$ and $a^{[l]}$ have the same dimension, so what you see in Resnets is a lot of use of same convolutions, so that the dimension is equal, coz the same convolution preserves dimensions. In case the input and output have different dimensions, for example $a^{[l]}$ is 128 and $a^{[l + 2]}$ is 256, what to do is add an extra matrix ($W_s$, (256 * 128) here). It could be a matrix or parameters learned by the nn, it could be a fixed matrix that just implements zero padding, \n",
    "\n",
    "$a^{[l + 2]} = g(z^{[l + 2]} + a^{[l]}) = g(W^{[l + 2]} a^{[l + 1]} + b^{[l + 2]} + W_s a^{[l]}) = a^{[l + 2]} = g(a^{[l]}) = a^{[l]}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
