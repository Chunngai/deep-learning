{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>**# 2.3 Residual Networks (ResNets)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very very deep nn are difficult to train because of **vanishing** and **exploding** gradients types of problems.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a^{[l]}$ -> Linear -> ReLU -> $a^{[l + 1]}$ -> Linear -> ReLU -> $a^{[l + 2]}$  \n",
    "  \n",
    "$z^{[l + 1]} = W^{[l + 1]} a^{[l]} + b^{[l + 1]}$  \n",
    "$a^{[l + 1]} = g(z^{[l + 1]})$  # g() is ReLU here  \n",
    "  \n",
    "$z^{[l + 2]} = W^{[l + 2]} a^{[l + 1]} + b^{[l + 2]}$  \n",
    "$a^{[l + 2]} = g(z^{[l + 2]})$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a residual net, we're going to make a change to the steps above. We take $a^{[l]}$ and just fast forward it, copy it much further into the neural network before applying to nonlinearity, the (second here) ReLU nonlinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a^{[l]}$ <font color=red>-></font> Linear -> ReLU -> $a^{[l + 1]}$ -> Linear <font color=red>-></font> ReLU -> $a^{[l + 2]}$    \n",
    "(from the red \"->\" to the red \"->\")\n",
    " \n",
    "  \n",
    "$z^{[l + 1]} = W^{[l + 1]} a^{[l]} + b^{[l + 1]}$  \n",
    "$a^{[l + 1]} = g(z^{[l + 1]})$  # g() is ReLU here  \n",
    "  \n",
    "$z^{[l + 2]} = W^{[l + 2]} a^{[l + 1]} + b^{[l + 2]}$  \n",
    "<font color=blue>$a^{[l + 2]} = g(z^{[l + 2]} + a^{[l]})$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a^{[l]}$ here makes a residual block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using residual blocks allows you to train much deeper neural networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why Resnets?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Std optimization algorithm to train a plain nn (without all the extra residual), as the number of layers increases, the training error will tend to decrease after a while, but then trend back up. In reality, having a plain nn that's very deep means that your optimization algorithm just has much harder time training, the training error gets worse if you pick a nn that's too deep. With Resnets even as the number of layers gets deeper, you can have a performance of the training error kind of keep on going down. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>**# 2.4 Why Resnets work**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the last video we know as the network gets deeper, it can hurt your ability to train the network to do well on the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a nn like this:  \n",
    "  \n",
    "X -> [Big NN] -> $a^{[l]}$  \n",
    "  \n",
    "And add two layers:  \n",
    "X -> [Big NN] -> $a^{[l]}$ <font color=red>-></font> Layer1 -> <font color=red>Layer2</font> -> $a^{[l+2]}$  \n",
    "  \n",
    "And use the ReLU func."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a^{[l + 2]} = g(z^{[l + 2]} + a^{[l]}) = g(W^{[l + 2]} a^{[l + 1]} + b^{[l + 2]} + a^{[l]})$  \n",
    "  \n",
    "If L2 regularization is used, if $W^{[l + 2]}$ = 0, and suppose b = 0,  \n",
    "$a^{[l + 2]} = g(a^{[l]}) = a^{[l]}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this shows is that the identity function is easy for residual block to learn. What that means is that adding these two layers in neural network, it does not really hurt your nn ability to do as well as the simpler nn without the two extra layers, coz it's quite easy for it to learn the identity func. It just copies $a^{[l]}$ to $a^{[l + 2]}$, despite the addition of these two layers. If the layers actually learn something useful, maybe they can do even better than learning the identity func. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main reason the residual network works is that it's so easy for these extra layers to learn the identity func that you're kind of guaranteed that it does not hurt performance and lot of time maybe even helps performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more detail. Assume that $z^{[l + 2]}$ and $a^{[l]}$ have the same dimension, so what you see in Resnets is a lot of use of same convolutions, so that the dimension is equal, coz the same convolution preserves dimensions. In case the input and output have different dimensions, for example $a^{[l]}$ is 128 and $a^{[l + 2]}$ is 256, what to do is add an extra matrix ($W_s$, (256 * 128) here). It could be a matrix or parameters learned by the nn, it could be a fixed matrix that just implements zero padding, \n",
    "\n",
    "$a^{[l + 2]} = g(z^{[l + 2]} + a^{[l]}) = g(W^{[l + 2]} a^{[l + 1]} + b^{[l + 2]} + W_s a^{[l]}) = a^{[l + 2]} = g(a^{[l]}) = a^{[l]}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<font size=3>**# 2.5 Network in Network and 1 \\* 1 convolutions**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 * 1 Convolution for Inputs with Multiple Channels**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose their is a 6 * 6 * 32 input and a 1 * 1 * 32 filter. What the 1 * 1 will do is it will look at each of the 36 diff pos, and it will take the element wise product btw 32 numbers on the left and the 32 numbers in the filter, and then apply a ReLU nonliearity to it after that. Values in diff channels in the filter can be viewd as weights here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It's Like a Full Connected nn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 1 * 1 filter with its input can be viewd as a fully connected nn. What the fully connected nn does is it inputs 32 numbers, and outputs number of filters outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It Can Be Used to Shrink the Number of Channels**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you want to shrink the number of channel to n, just use n 1 * 1 filters. It can also be used to increase the number of channels. If the number of filters is the same as the channel number, it just adds nonlinearity.<font color=red># why it can add nonlinearity? use the ReLU after the convolution?</font>\n",
    "\n",
    "Note that pooling is used to shrink the height and width of the input."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>**# 2.6 Inception network motivation**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What Inception Network Does**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It helps you decide the size of filters, whether to use a conv or pooling layer or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How it Works**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you want to decide the size of filters or a pooling layer. You first use 64 1 * 1 * 192 filters and get an 28 * 28 * 64 output. Then 128 3 * 3 * 128 filters, another output 28 * 28 * 128. You then stack the outputs together. Then you use 32 5 * 5 * 192 filters, and a padded MAX-POOL layer with the output 28 * 28 * 32. After this you stack all the inputs together. Then you get an output with 64 + 128 + 32 + 32 channels. Note that all filters use the same conv so that the width and height of the outputs are the same. And the output of the pooling should match those of other outputs. In a word the width and height of all outputs should be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**the Computation Cost**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the 5 * 5 filter as an example. \n",
    "32 5 * 5 * 192 filters and the output is 28 * 28 * 32. Thus in total (28 * 28 * 32) * (5 * 5 * 192) = 120 million multiplication times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use 1 * 1 Filters to Reduce the Computation Cost**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28 * 28 * 192 -> (16 1 * 1 * 192) (bottleneck layer) -> 28 * 28 * 32\n",
    "\n",
    "cost for the first layer: (28 * 28 * 16) * (1 * 1 * 192) = 2.4 million.\n",
    "the sceond: (28 * 28 * 32) * (5 * 5 * 16) = 10 million.\n",
    "The sum: 12.4 m, which is the one tenth of the 120 m."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Will the Performance Be Hurt?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So long as you implement the bottleneck layer within the region （合理地）, you can shrink down the repr size significantly and does not hurt the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>**# 2.8 Inception Network**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 1 * 1 and 3 * 3 filter do the same things, maybe with different number of 1 * 1 filters. And for the pooling, the number of channels of the normal output will be 192, the same as the input. What we can do is add 32 1 * 1 * 192 filter after the pooling to get an output with less channels. And stack the outputs together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>**# 2.9 Transfer Learning**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use weights already trained.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train your own softmax func**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you have little data. Delete the softmax layer of the original code and write your own softmax. Train parameters associated with your own softmax layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Store the activation values of the input of the softmax func to disk**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the layers before your own softmax func as a fixed func and use this fixed func to get the activation values of the input data which are to be the input of the softmax func and store them to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you have a larger training set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freeze fewer layers. Such as freeze some in the former part and train the latter part. Take the weights in the last few layers as initialization for GD or delete them and train your own hidden layers.\n",
    "\n",
    "The more data you have, the less layers you could freeze.\n",
    "\n",
    "If you have very large data, use the weights as initialization and train the whole model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>**# 2.10 Data augmentation**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mirroring\n",
    "Random Cropping\n",
    "Rotation\n",
    "Shearing\n",
    "Local warping\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Color shifting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Implementing distortions during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
