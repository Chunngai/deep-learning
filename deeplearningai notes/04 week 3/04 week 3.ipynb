{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>**# 3.1 Object localization**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification with Localization vs. Detection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification and the classification of localization: usually one obj. Detection: there can be multiple objs in an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output Bounding a Box to Localize**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bisides features such as pedestrian, car, motorcycle and background, add a few output numbers: bx, by, bh, bw. And the training set should contain four numbers giving the bounding box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Notations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "upper left: (0, 0), lower right: (1, 1)  \n",
    "(bx, by): the midpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining the Target Label y**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have four classes:  \n",
    "1. pedestrian  \n",
    "2. car  \n",
    "3. motorcycle  \n",
    "4. background  \n",
    "  \n",
    "And we need to output $b_x, b_y, b_h, b_w$, class label (1 - 4)  \n",
    "  \n",
    "$$\n",
    "\\boldsymbol y = \n",
    "\\begin{bmatrix}\n",
    "\\begin{aligned}\n",
    "p_c \\\\\n",
    "b_x \\\\\n",
    "b_y \\\\\n",
    "b_h \\\\\n",
    "b_w \\\\\n",
    "c_1 \\\\\n",
    "c_2 \\\\\n",
    "c_3\n",
    "\\end{aligned}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$p_c$ denotes whether there is any object. And $c_1, c_2, c_3$ at most one of them can be equal to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L(\\boldsymbol {\\hat y}, y) = \n",
    "\\left \\{\n",
    "\\begin{aligned}\n",
    "& (\\boldsymbol{\\hat y_1} - \\boldsymbol y_1)^2 + (\\boldsymbol{\\hat y_2} - \\boldsymbol y_2)^2 + \\dots + (\\boldsymbol{\\hat y_8} - \\boldsymbol y_8)^2 & y_1 = 1 (p_c = 1) \\\\\n",
    "& (\\boldsymbol{\\hat y_1} - \\boldsymbol y_1)^2 & y_1 = 0 (p_c = 0)\n",
    "\\end{aligned}\n",
    "\\right .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice $p_c$: logistic reg loss, $b_x, b_y, b_h, b_w$: squared error, $c_1, c_2, c_3$: softmax func loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>**# 3.2 Landmark detection**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Landmark Detection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By selecting a number of landmarks, and generating a label training set that contains all of these landmarks, you can then have the nn tall you where are all the key positions or the key landmarks on a face.\n",
    "\n",
    "$$ \\boldsymbol y = \n",
    "\\begin{bmatrix}\n",
    "\\begin{aligned}\n",
    "face? \\\\\n",
    "l_{1x} \\\\\n",
    "l_{1y} \\\\\n",
    "\\vdots \\\\\n",
    "l_{64x} \\\\\n",
    "l_{64y}\n",
    "\\end{aligned}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>**# 3.3 Object detection**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cropped imgs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you are to build a car detection algorithm. Build a labeled training set with closely cropped examples of cars. For our purposes in this training set you can start off with closely cropped images, meaning that $\\boldsymbol x$ is pretty much only the car. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sliding Windows Detection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically go through every region of the size of the window, and pass lots of little cropped images into the conv net and make classification. Use diff sizes of windows then so long as there's a car somewhere in the img, there will be a window can detect it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disadvantage**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>**# 3.4 Convolutional Implementation of Sliding Windows**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Turing FC Layer into Convolutional Layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 * 5 * 16 -> (FC) -> a 400-unit fc layer -> (FC) -> a 400-unit fc layer -> y (softmax 4(4))  \n",
    "â†“  \n",
    "5 * 5 * 16 -> (FC 400 5 * 5 * 16) -> 1 * 1 * 400 -> (FC 400 1 * 1 * 400) -> 1 * 1 * 400 -> (4 1 * 1 * 400) -> 1 * 1 * 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolution Implementation of Sliding Windows**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of running forward prop on four subsets of the input img independently (using cropped small images), use the entire img to detect with turning fc layers into conv layers.  \n",
    "  \n",
    "Say that your conv net inputs 14 * 14 * 3 imgs, and your test set img is 16 * 16 * 3 (this img is generated by padding 2 rows at the bottom and 2 cols on the right).  \n",
    "  \n",
    "For the input 14 * 14 * 3 -> 1 * 1 * 4,\n",
    "for the output 16 * 16 * 3 (stride = 2 in the test set img, max pool: 2 * 2) -> 2 * 2 * 4. (0, 0) is the output of [0:14, 0:14] in the input img in the test set, and (0, 1) the output of [2:16, 2:16], (1, 0) [2:16, 0:14], (1, 1) the output of [2:16, 2:16]. In this example [0:14, 0:14] is 1 and the others 0.\n",
    "\n",
    "Because of the max pooling of 2 (max pool: 2 * 2) that this corresponds to running your nn with a stride of 2 on the original img. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weakness**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pos of the bouding boxes is not goling to be too accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>**# 3.5 Bounding box predictions**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOLO Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you have a 100 * 100 img, you're going to place down a grid on this img. A 3 * 3 grid is used here. In actual implementation you use a finer one, maybe a 19 * 19 grid. The basic idea is you're going to take the img classification and localization algorithm that you saw in the first video of this week and apply that to each of the nine grid cells of this image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Labels for training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each grid cell:  \n",
    "$$\n",
    "\\boldsymbol y = \n",
    "\\begin{bmatrix}\n",
    "\\begin{aligned}\n",
    "p_c \\\\\n",
    "b_x \\\\\n",
    "b_y \\\\\n",
    "b_h \\\\\n",
    "b_w \\\\\n",
    "c_1 \\\\\n",
    "c_2 \\\\\n",
    "c_3\n",
    "\\end{aligned}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "  \n",
    "9 $\\boldsymbol {\\hat y}$ for the img."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What YOLO Does**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The img has 2 objs and what the YOLO algorithm does is it takes the midpoint of each of the 2 objs, and it assigns the obj to the grid cell containing the midpoint. And so even though the central grid cell has some parts of both cars, we will pretent the central grid cell has no intersting obj.  \n",
    "  \n",
    "The target output will be 3 * 3 * 8.  \n",
    "  \n",
    "Note that the algorithm runs only once for each example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantage**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outputs precise bouding boxes. So long as you don't have more than one obj in each grid cell, this algorithm should work ok.  \n",
    "  \n",
    "Very fast because it uses convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specify the Bounding Boxes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each grid the top left is (0, 0) and the low right is (1, 1). $b_x, b_y, b_h, b_w$ are specified **relative to** the grid cell.  \n",
    "   \n",
    "The height, width of the bounding box is specified as a fraction of the overall height, width of the box (grid). For example the width of the bounding box is 90% of the width of the grid, so $b_w$ is 0.9. And the height of the bounding box is 50% of the height of the grid, in that case $w_b$ is 0.5.  \n",
    "  \n",
    "$b_x, b_y$ must be btw 0 and 1 coz pretty much by definition the midpoint is within the bounds of that grid cell it was assigned to if it was not btw 0 and 1 if it's outside the square it would be assigined to a diff grid cell.  \n",
    "  \n",
    "$b_w and b_h$ can be greater than 1. For example there is an obj with a bounding box whose width and / or height are / is greater than those / that of the grid cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>**# 3.6 Intersection over union**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$IoU = \\frac{size \\ of \\ intersection}{size \\ of \\ union}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Correct\" if IoU $\\geq$ 0.5\n",
    "  \n",
    "0.5 can be replaced by other values such as 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>**# 3.7 Non-max suppression**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're implementing the detection algorithm maybe grids aroung the grid containing the midpoint also think they contain the midpoint so that the obj is detected more than once. For example there are two objs in an img more than two grid will give a vector with $p_c = 1$. Non-max suppression is a way to make sure your algorithm detects each obj only once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How it Works**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the bounding box with the highest $p_c$ will be picked and the IoU between the box and other bounding boxes will be calculated. And boxes that give a high IoU will be suppressed because this means that the picked box overlaps a lot with boxes giving high IoUs. And the process is repeated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non-max suppression algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we just detect cars now. Thus each output prediction is:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\begin{aligned}\n",
    "p_c \\\\\n",
    "b_x \\\\\n",
    "b_y \\\\\n",
    "b_h \\\\\n",
    "b_w\n",
    "\\end{aligned}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Discard all boxes with $p_c \\leq 0.6$\n",
    "While there are any remaining boxes:  \n",
    "&emsp;Pick the box with the largest $p_c$, output that as a prediction  \n",
    "&emsp;Discard any remaining box with $IoU \\geq 0.5$ with the box output in the previous step.  \n",
    "  \n",
    "If multiple objs are to be detected, independently carry out non-max suppression multiple times, one on each of the outputs classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>**# 3.8 Anchor Boxes**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Makes a grid cell detect multiple objs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that the midpoint of the pedestrian and the midpoint of the car are in almos the same place and both of them fall into the same grid cell. For that grid cell if y outputs the $\\boldsymbol {\\hat y}$ where you're detecting three classes it won't be able to output the detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What Anchor Boxes Do**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the example above predifine two diff shapes called anchor boxes with different shapes, such as a fat short one and a slim tall one. And associate two predictions with the two anchor boxes. In general you might use more anchor boxes maybe 5 or even more.  \n",
    "  \n",
    "If you're using two anchor boxes, then each of the nine grid cells you get two predicted bounding boxes, some of them will have very low probability very low $p_c$, but you still get two predicted bounding boxes for each of the nine grid cell. Some of the bounding boxes can go outside the height and width of the grid cell that it came from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Output Vector**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the vector introduced in the previous videos, use\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\begin{aligned}\n",
    "p_c \\\\\n",
    "b_x \\\\\n",
    "b_y \\\\\n",
    "b_h \\\\\n",
    "b_w \\\\\n",
    "c_1 \\\\\n",
    "c_2 \\\\\n",
    "c_3 \\\\\n",
    "p_c \\\\\n",
    "b_x \\\\\n",
    "b_y \\\\\n",
    "b_h \\\\\n",
    "b_w \\\\\n",
    "c_1 \\\\\n",
    "c_2 \\\\\n",
    "c_3\n",
    "\\end{aligned}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The first eight rows are for anchor box 1 and the remaining rows for anchor box 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Anchor Box Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With two anchor boxes:  \n",
    "Each objs in training image is assigned to grid cell that contains obj's midpoint and anchor box for the grid cell with highest IoU.  \n",
    "  \n",
    "Output now: 3 * 3 * (8 * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose anchor box 1 is a rectangle whose height is longer and anchor box whose width is longer. If there is a grid contains the midpoints of both a car and a pedestrian, the output vector will be\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\begin{aligned}\n",
    "1 \\\\\n",
    "b_x \\\\\n",
    "b_y \\\\\n",
    "b_h \\\\\n",
    "b_w \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "b_x \\\\\n",
    "b_y \\\\\n",
    "b_h \\\\\n",
    "b_w \\\\\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0\n",
    "\\end{aligned}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "  \n",
    "And if the grid only contains a pedestrian:  \n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\begin{aligned}\n",
    "0 \\\\\n",
    "? \\\\\n",
    "? \\\\\n",
    "? \\\\\n",
    "? \\\\\n",
    "? \\\\\n",
    "? \\\\\n",
    "? \\\\\n",
    "1 \\\\\n",
    "b_x \\\\\n",
    "b_y \\\\\n",
    "b_h \\\\\n",
    "b_w \\\\\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0\n",
    "\\end{aligned}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It cannot handle the case that 3 objs are in the same grid cell and the case that 2 objs with the same anchor shape are in the same grid cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to Choose Anchor Boxes?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By hand or K-means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>**# 3.10 Region Proposals**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Region Proposal: R-CNN (Regions with Convolutional Networks)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It tries to pick just a few regions that make sense to run your conv net classifier on. So rather than running your sliding windows on every single window, you instead select just a few windows, and run your conv net classifier on just a few windows. The way they perform the region proposals is to run an algorithm called a segmentation algorithm. The R-CNN algorithm does not just trust the bounding box it was given, it also outputs a bounding box $b_x, b_y, b_h, b_w$ in order to get a more accurate bounding box than whatever happended to surround the block that the img segmentation algorithm gave it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Downside**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slow. Classify the regions one at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fast R-CNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faster, use conv implementation of sliding windows to classify all the proposed regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Faster R-CNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more faser, use a conv nn to propose regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improved R-CNN alrotihms are faster but still slower than YOLO. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
